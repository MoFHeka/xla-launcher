/*
 * @copyright
 * BSD 3-Clause License, 2025, He Jia <mofhejia@163.com>
 * BSD 3-Clause License, 2023, pytorch-tpu
 */

#pragma once

#ifndef XLA_LAUNCHER_RUNTIME_COMPUTATION_CLIENT_HPP
#define XLA_LAUNCHER_RUNTIME_COMPUTATION_CLIENT_HPP

#include <cstdint>
#include <functional>
#include <map>
#include <memory>
#include <optional>
#include <string>
#include <unordered_map>
#include <utility>
#include <variant>
#include <vector>

#include "xla/hlo/builder/xla_computation.h"
#include "xla/pjrt/pjrt_client.h"
#include "xla/shape_util.h"
#include "xla_launcher/device.hpp"
#include "xla_launcher/runtime/debug_macros.hpp"
#include "xla_launcher/runtime/tensor_source.hpp"
#include "xla_launcher/runtime/xla_util.hpp"

namespace xla_launcher {
namespace runtime {

using hash_t = util::hash_t;

using ClientOptions = xla_launcher::ClientOptions;

struct ClientExecuteOptions {
  // If options.explode_tuple is true, the output tuple will be decomposed into
  // its single elements.
  bool explode_tuple{true};
};

class Data {
 public:
  using Handle = int64_t;

  Data(std::string device, xla::Shape shape, bool should_donate_buffer = false)
    : xla_device_(device),
      xla_shape_(std::move(shape)),
      should_donate_buffer_(should_donate_buffer) {}

  virtual ~Data() {}

  const std::string& device() const { return xla_device_; }

  const xla::Shape& shape() const { return xla_shape_; }

  bool should_donate_buffer() const { return should_donate_buffer_; }

  void set_should_donate_buffer(bool should_donate_buffer) {
    should_donate_buffer_ = should_donate_buffer;
  }

  virtual std::string ToString() const = 0;

  virtual Handle GetHandle() = 0;

  virtual void Assign(const Data& data) = 0;

  virtual bool HasValue() const = 0;

  virtual bool HasSharding() const = 0;

  virtual xla::OpSharding GetSharding() const = 0;

 private:
  std::string xla_device_;
  xla::Shape xla_shape_;
  bool should_donate_buffer_;
};

// TODO(He Jia): Support all possible compile options if needed.
struct CompileInstance {
  CompileInstance() = default;
  CompileInstance(
    xla::XlaComputation computation, std::string compilation_device,
    std::vector<std::string> devices, const xla::Shape* output_shape,
    bool parameter_is_tupled_arguments = false, bool is_sharded = false,
    bool allow_spmd_sharding_propagation_to_output = true,
    bool use_auto_spmd_partitioning = false,
    std::vector<int64_t> auto_spmd_mesh_shape = {},
    std::vector<int64_t> auto_spmd_mesh_ids = {})
    : computation(std::move(computation)),
      compilation_device(std::move(compilation_device)),
      devices(std::move(devices)),
      output_shape(output_shape),
      parameter_is_tupled_arguments(parameter_is_tupled_arguments),
      is_sharded(is_sharded),
      allow_spmd_sharding_propagation_to_output(
        allow_spmd_sharding_propagation_to_output),
      use_auto_spmd_partitioning(use_auto_spmd_partitioning),
      auto_spmd_mesh_shape(auto_spmd_mesh_shape),
      auto_spmd_mesh_ids(auto_spmd_mesh_ids) {}

  xla::XlaComputation computation;
  std::string compilation_device;
  std::vector<std::string> devices;
  const xla::Shape* output_shape = nullptr;
  bool parameter_is_tupled_arguments;
  bool is_sharded;
  bool allow_spmd_sharding_propagation_to_output;
  bool use_auto_spmd_partitioning;
  std::vector<int64_t> auto_spmd_mesh_shape;
  std::vector<int64_t> auto_spmd_mesh_ids;
};

// There are 3 different Computation class being used here
// 1. runtime::ComputationClient::Computation represent a computation from the
// ComputationClient perspective. It wraps a xla::XlaComputation and a vector
// of device.
// 2. xla::XlaComputation represent a xla computation, it is generated by the
// xla compiler.
// 3. xla::PjRtComputationClient::PjRtComputation which inherits from
// runtime::ComputationClient::Computation and contains a handle to represent
// the compiled program.
class Computation {
 public:
  // Our Computation is being used for 3 different purpose.
  // 1. To represent a xla computation build by xla_op_builder, in which case
  //    we would need the name and hash. Computation would be a wrapper around
  //    a runtime::ComputationClient::Computation.
  //    runtime::ComputationClient::Computation::devices_ would be empty.
  // 2. To represent a computation built by syncTensor and needs to be
  //    compiled.
  //    ...
  Computation(
    std::string name, xla::XlaComputation computation,
    std::vector<std::string> devices = {})
    : name_(name),
      computation_(std::move(computation)),
      devices_(std::move(devices)) {
    program_shape_ = ConsumeValue(computation_.GetProgramShape());
    const xla::HloModuleProto& proto = computation_.proto();
    hash_ = ConsumeValue(ComputeHash(proto, name));
  }

  Computation(
    std::string name, xla::XlaComputation computation, xla::PjRtDevice& device)
    : Computation(
      name, std::move(computation),
      std::vector<std::string>{std::string(device.ToString())}) {}

  // ...
  // 3. To represent a computation that is already compiled. In this case
  //    name_ and hash_ are not required. Computation will be a wrapper around
  //    an executable, PjRtComputationClient::PjRtComputation in our case. It
  //    is not ideal to use same class for 3 different purposes but this is
  //    the path took by upstream ltc.
  Computation(xla::XlaComputation computation, std::vector<std::string> devices)
    : Computation("", std::move(computation), std::move(devices)) {}

  explicit Computation(xla::XlaComputation computation)
    : Computation(std::move(computation), {}) {}

  virtual ~Computation() {}

  const std::string& name() const { return name_; }

  std::string get_device_string() const {
    // Assume that a xla_client_computation_ only contains one device for now.
    XLA_CHECK_EQ(devices().size(), 1);
    return devices()[0];
  }

  const xla::XlaComputation& computation() const {
    if (computation_moved_) {
      XLA_ERROR() << "Computation has been moved\n";
    }
    return computation_;
  }

  // We don't want to make a copy when passing computation_ to the runtime.
  // Class member will be accessed as const& and `xla::XlaComputation`
  // explictly delete its const& copy constructor so we have to const cast
  // here.
  xla::XlaComputation move_computation() {
    if (computation_moved_) {
      XLA_ERROR() << "Computation has been moved\n";
    }
    computation_moved_ = true;
    return std::move(const_cast<Computation*>(this)->computation_);
  }

  const xla::ProgramShape& program_shape() const { return program_shape_; }

  const hash_t& hash() const { return hash_; }

  virtual int parameters_size() const {
    return program_shape().parameters_size();
  }

  virtual const std::vector<xla::Shape>& parameter_shapes() const {
    return program_shape_.parameters();
  }

  virtual const std::vector<std::string>& parameter_names() const {
    return program_shape_.parameter_names();
  }

  virtual const xla::Shape& result_shape() const {
    return program_shape_.result();
  }

  const std::vector<std::string>& devices() const { return devices_; }

  virtual const std::string to_string() const {
    xla::HloModuleConfig hlo_config(program_shape());
    std::unique_ptr<xla::HloModule> module = ConsumeValue(
      xla::HloModule::CreateFromProto(computation().proto(), hlo_config));
    return module->ToString();
  }

  virtual const std::string get_memory_info() const {
    XLA_ERROR() << "Unimplemented";
    return "";
  }

 private:
  std::string name_;

  xla::XlaComputation computation_;
  xla::ProgramShape program_shape_;
  std::vector<std::string> devices_;
  bool computation_moved_ = false;

  hash_t hash_;

  // Computes a hash for an HLO module using deterministic proto
  // serialization. It ensures consistent ordering of Map fields and repeated
  // elements during during serialization. The resulting hash combines the
  // serialized module with its computation name.
  static absl::StatusOr<hash_t> ComputeHash(
    xla::HloModuleProto proto, const std::string& name) {
    for (auto& computation : *proto.mutable_computations()) {
      for (auto& instruction : *computation.mutable_instructions()) {
        instruction.mutable_metadata()->Clear();
      }
    }
    TF_ASSIGN_OR_RETURN(
      auto serialized_status,
      util::GetDeterministicSerializedModuleProto(proto));
    return util::MHash(name, serialized_status);
  }
};

class ComputationClient {
 public:
  using DataPtr = std::shared_ptr<Data>;
  using ComputationPtr = std::shared_ptr<Computation>;

  struct ExecuteComputationOptions : public ClientExecuteOptions {};

  struct ExecuteReplicatedOptions : public ClientExecuteOptions {};

  struct MemoryInfo {
    int64_t bytes_used = 0;
    int64_t bytes_limit = 0;
    int64_t peak_bytes_used = 0;
  };

  virtual ~ComputationClient() = default;

  // Creates a Data object with no actual device handle in it. The device handle
  // will be populated in an asynchrounous function.
  virtual DataPtr CreateDataPlaceholder(
    std::string device, xla::Shape shape,
    std::optional<xla::OpSharding> sharding = std::nullopt) = 0;

  // Returns data shards. We expect this to be called on PjRtShardedData to
  // retrieve the shards. If other data type is passed, it returns the input
  // wrapped inside a vector.
  virtual std::vector<DataPtr> GetDataShards(DataPtr data) {
    XLA_ERROR() << "Unimplemented";
    return {};
  }

  // Returns data shard at a given index.
  virtual DataPtr GetDataShard(DataPtr data, size_t index) {
    XLA_ERROR() << "Unimplemented";
    return nullptr;
  }

  // Returns wrapped data shards as PjRtShardedData.
  virtual DataPtr WrapDataShards(
    absl::Span<const DataPtr> shards, std::string device, xla::Shape shape,
    xla::OpSharding sharding) {
    XLA_ERROR() << "Unimplemented";
    return nullptr;
  }

  // Returns OpSharding attached to PjRtShardedData. The returned optional
  // structure will be empty if there is no sharding, like with PjRtData.
  virtual std::optional<xla::OpSharding> GetDataSharding(DataPtr handle) {
    XLA_ERROR() << "Unimplemented";
    return std::nullopt;
  }

  virtual std::string PjRtDeviceToString(
    xla::PjRtDevice* const device) const = 0;

  // Transfers local tensor values to the GPU devices and fetches the handles.
  virtual std::vector<DataPtr> TransferToDevice(
    absl::Span<const std::shared_ptr<const TensorSource>> tensors) {
    XLA_ERROR() << "Unimplemented";
    return {};
  }

  // Reshard and return data sharded by `sharding` spec. This is a no-op if the
  // input sharding spec is identical to the target `sharding` sharding spec.
  virtual std::vector<DataPtr> ReshardData(
    absl::Span<const DataPtr> handles,
    absl::Span<const xla::OpSharding> shardings) {
    XLA_ERROR() << "Unimplemented";
    return {};
  }

  // Transfers local sharded tensor values to the TPU devices and returns a
  // `PjRtShardedData`.
  virtual DataPtr TransferShardsToDevice(
    absl::Span<const std::shared_ptr<const TensorSource>> tensor_shards,
    std::string device, xla::Shape shape, xla::OpSharding sharding) {
    XLA_ERROR() << "Unimplemented";
    return nullptr;
  }

  // Copies `data->buffer` to `dst` device buffer.
  virtual DataPtr CopyToDevice(DataPtr data, std::string dst) = 0;

  // Reads the tensor literal values stored at TPU server sites, behind the
  // supplied handles.
  // Note: `TransferFromDevice` call will block until the `DataPtrs` are ready
  // if they were created by `TransferToDevice` or `Execute*`. Calling this from
  // python while holding the GIL can cause deadlocks!
  virtual std::vector<xla::Literal> TransferFromDevice(
    absl::Span<const DataPtr> handles) {
    XLA_ERROR() << "Unimplemented";
    return {};
  }

  virtual std::uintptr_t UnsafeBufferPointer(const DataPtr handle) = 0;

  virtual std::shared_ptr<xla::PjRtBuffer> GetPjRtBuffer(
    const DataPtr handle) const = 0;

  // Compiles StableHlo bytecode to XlaComputation.
  virtual xla::XlaComputation CompileStableHlo(
    const std::string& bytecode,
    std::function<void(mlir::ModuleOp&, mlir::MLIRContext& context)>
      canonicalize_fn = nullptr);

  // Compiles a set of computations.
  virtual std::vector<ComputationPtr> Compile(
    std::vector<CompileInstance> instances) = 0;

  // Serialize a computation to a string.
  virtual std::string SerializeComputation(
    const ComputationPtr computation) const = 0;

  // Deserialize a string resulting from SerializeComputation back to a
  // Computation. If the deserialization fails, nullptr is returned.
  virtual ComputationPtr DeserializeComputation(
    const std::string& serialized) const = 0;

  // Returns a hash of the current compilation environment.
  virtual hash_t HashCompilationEnv() = 0;

  // Executes computation with arguments and returns the result.
  // The passed device must match the common device of the arguments Data.
  // If options.explode_tuple is true, the output tuple will be decomposed into
  // its single elements.
  virtual std::vector<DataPtr> ExecuteComputation(
    const Computation& computation, absl::Span<const DataPtr> arguments,
    const std::string& device,
    const ExecuteComputationOptions& options = ExecuteComputationOptions{}) = 0;

  // Executes the computation on multiple local devices in parallel.
  // Each argument to the executable is expected to be sharded in the same order
  // as `devices`. If options.explode_tuple is true, the output tuples will be
  // decomposed into their single elements. Returns a vector of outputs, each
  // of which is sharded in the same order as `devices`.
  virtual std::vector<DataPtr> ExecuteReplicated(
    const Computation& computation, absl::Span<const DataPtr> arguments,
    absl::Span<const std::string> devices,
    const ExecuteReplicatedOptions& options) {
    XLA_ERROR() << "Unimplemented";
    return {};
  }

  virtual std::string GetDefaultDevice() const = 0;

  virtual DeviceType GetDeviceType() const = 0;

  virtual std::string GetDeviceKind(const std::string& device) = 0;

  virtual xla::PjRtPlatformId GetPlatformID() const = 0;

  virtual absl::StatusOr<xla::PjRtDevice*> LookupAddressableDevice(
    int local_device_id) const = 0;

  virtual std::intptr_t GetCudaStreamForDevice(int local_device_id) const = 0;

  virtual size_t GetNumLocalDevices() const = 0;

  virtual size_t GetNumDevices() const = 0;

  virtual std::vector<std::string> GetLocalDevices() const = 0;

  virtual std::vector<std::string> GetAllDevices() const = 0;

  virtual int GetProcessIndex() const = 0;

  virtual int GetNumProcesses() const = 0;

  using DeviceAttribute =
    std::variant<std::string, bool, int64_t, std::vector<int64_t>, float>;

  virtual const std::unordered_map<
    std::string, ComputationClient::DeviceAttribute>
  GetDeviceAttributes(const std::string& device) = 0;

  virtual void SetReplicationDevices(
    std::shared_ptr<std::vector<std::string>> devices) {
    XLA_ERROR() << "Unimplemented";
  }

  virtual std::shared_ptr<std::vector<std::string>> GetReplicationDevices() {
    XLA_ERROR() << "Unimplemented";
    return nullptr;
  }

  // virtual std::map<std::string, Metric> GetMetrics() const = 0;

  virtual MemoryInfo GetMemoryInfo(const std::string& device) = 0;

  // Block until pass in devices' async operation are finished. If empty, all
  // the local devices will be waited for.
  virtual void WaitDeviceOps(
    absl::Span<const std::string> devices = {}) const = 0;

  /*
  // Check whether the XlaCoordinator has been initialized.
  virtual bool CoordinatorInitialized() const = 0;

  // Initialize the XlaCoordinator for the runtime.
  virtual void InitializeCoordinator(
    int global_rank, int world_size, std::string master_addr,
    std::string port) = 0;

  // Return the XlaCoordinator for the runtime.
  virtual XlaCoordinator& GetCoordinator() = 0;
  */

  virtual void RegisterCustomCall(
    const std::string& fn_name, void* function_ptr,
    const std::string& platform) = 0;

  // Installs a callback to be called when the buffer backing `data` is ready.
  virtual void OnReadyCallback(
    DataPtr data, const std::function<void()>& callback) = 0;

  // Utility API around the vector based Compile() API to compile a single
  // computation.
  ComputationPtr Compile(
    xla::XlaComputation computation, std::string compilation_device,
    std::vector<std::string> devices, const xla::Shape* output_shape);

  // Retrieves the set of devices to be passed to the computation client
  // Compile() API. If the devices array is empty, a vector with the single
  // device will be returned. Otherwise a vector with the devices content will
  // be returned.
  std::vector<std::string> GetCompilationDevices(
    const std::string& device, absl::Span<const std::string> devices);

  // Retrieves the ordinal number out of a device string. This is the number
  // after the last ':' character of the device string.
  static int64_t GetDeviceOrdinal(const std::string& device);

 protected:
  static constexpr auto spmd_device_str = "SPMD:0";

  // Metrics common to all client interfaces.
  // static metrics::Metric* TransferToDeviceMetric();
  // static metrics::Metric* TransferToDeviceTransformMetric();
  // static metrics::Metric* TransferFromDeviceMetric();
  // static metrics::Metric* CompileMetric();
  // static metrics::Metric* EagerCompileMetric();
  // static metrics::Metric* ExecuteMetric();
  // static metrics::Metric* EagerExecuteMetric();
  // static metrics::Metric* ExecuteReplicatedMetric();
  // static metrics::Metric* ExecuteParallelMetric();
  // static metrics::Metric* ExecuteChainedMetric();
  // static metrics::Metric* DeconstructTupleMetric();
  // static metrics::Counter* CreateAsyncDataHandlesCounter();
  // static metrics::Counter* CreateDataHandlesCounter();
  // static metrics::Counter* ReleaseDataHandlesCounter();
  // static metrics::Counter* DestroyDataHandlesCounter();
  // static metrics::Metric* ReleaseDataHandlesTimeMetric();
  // static metrics::Counter* CreateCompileHandlesCounter();
  // static metrics::Counter* ReleaseCompileHandlesCounter();
  // static metrics::Counter* DestroyCompileHandlesCounter();
  // static metrics::Metric* ReleaseCompileHandlesTimeMetric();
  // static metrics::Counter* StableHloCompileCounter();
  // static metrics::Metric* InboundDataMetric();
  // static metrics::Metric* OutboundDataMetric();
};

}  // namespace runtime
}  // namespace xla_launcher

#endif  // XLA_LAUNCHER_RUNTIME_COMPUTATION_CLIENT_HPP
